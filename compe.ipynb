{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Romira915/jds_compe/blob/main/compe.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IM5uLIQxeh7q",
        "outputId": "229c25dd-3c63-4227-d0f4-bdd0c7cfbfdc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: mecab-python3 in /usr/local/lib/python3.7/dist-packages (1.0.5)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.20.1)\n",
            "Requirement already satisfied: fugashi in /usr/local/lib/python3.7/dist-packages (1.1.2)\n",
            "Requirement already satisfied: ipadic in /usr/local/lib/python3.7/dist-packages (1.0.0)\n",
            "Requirement already satisfied: torch_optimizer in /usr/local/lib/python3.7/dist-packages (0.3.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.8.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.12.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: pytorch-ranger>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from torch_optimizer) (0.1.1)\n",
            "Requirement already satisfied: torch>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from torch_optimizer) (1.11.0+cu113)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.6.15)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n"
          ]
        }
      ],
      "source": [
        "  !pip install mecab-python3 transformers fugashi ipadic torch_optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "MlRdjjoxgfeq"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "import time\n",
        "import random\n",
        "\n",
        "import MeCab\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import transformers\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from torch import Tensor, cuda, optim\n",
        "from torch.nn import functional as F\n",
        "import torch_optimizer\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "import math\n",
        "from torch.autograd.function import InplaceFunction\n",
        "from torch.nn import Parameter\n",
        "import torch.nn.init as init\n",
        "from transformers import (AutoModel, AutoTokenizer, BertJapaneseTokenizer,\n",
        "                          BertModel, BartForSequenceClassification, BertConfig, DistilBertConfig, DistilBertTokenizer, DistilBertModel, AlbertConfig, AlbertModel, AlbertForMaskedLM, AlbertTokenizerFast)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_q3cDMvEfO1S",
        "outputId": "abb8fc71-cf17-49cc-bc14-2f3dd7e41467"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "train_path = \"train.csv\"\n",
        "test_path = \"test.csv\"\n",
        "compe_path = \"compe.csv\"\n",
        "submission_path = \"submission.csv\"\n",
        "\n",
        "try:\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')\n",
        "\n",
        "  compe_dir = \"/content/drive/My Drive/Documents/compe/\"\n",
        "\n",
        "  train_path = compe_dir + train_path\n",
        "  test_path = compe_dir + test_path\n",
        "  compe_path = compe_dir + compe_path\n",
        "  submission_path = compe_dir + submission_path\n",
        "\n",
        "except ImportError:\n",
        "  pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "-30wKlqKd1dU"
      },
      "outputs": [],
      "source": [
        "SEED = 22109"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "AG2fQszTgim5"
      },
      "outputs": [],
      "source": [
        "train_df = pd.read_csv(train_path)\n",
        "test_df = pd.read_csv(test_path)\n",
        "compe_df = pd.read_csv(compe_path)\n",
        "\n",
        "train_df, valid_df = train_test_split(\n",
        "    train_df, test_size=0.2, shuffle=True, random_state=SEED)\n",
        "\n",
        "# train_df = pd.concat([train_df, test_df], axis=0)\n",
        "\n",
        "train_text = train_df[\"text\"].values.astype('U')\n",
        "valid_text = valid_df[\"text\"].values.astype('U')\n",
        "test_text = test_df[\"text\"].values.astype('U')\n",
        "y = train_df[\"label\"].values.astype(\"int8\")\n",
        "valid_y = valid_df[\"label\"].values.astype(\"int8\")\n",
        "test_y = test_df[\"label\"].values.astype(\"int8\")\n",
        "compe_text = compe_df[\"text\"].values.astype('U')\n",
        "\n",
        "# model_name = \"nlptown/bert-base-multilingual-uncased-sentiment\"\n",
        "# model_name = \"daigo/bert-base-japanese-sentiment\"\n",
        "model_name = \"cl-tohoku/bert-base-japanese-whole-word-masking\"\n",
        "# model_name = \"ken11/albert-base-japanese-v1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "ISZnBa7kgnEo"
      },
      "outputs": [],
      "source": [
        "# Datasetの定義\n",
        "class CreateDataset(Dataset):\n",
        "    def __init__(self, X, y, tokenizer, max_len):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):  # len(Dataset)で返す値を指定\n",
        "        return len(self.y)\n",
        "\n",
        "    def __getitem__(self, index):  # Dataset[index]で返す値を指定\n",
        "        text = self.X[index]\n",
        "        inputs = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=512,\n",
        "            pad_to_max_length=True,\n",
        "            # padding=True, \n",
        "            truncation=True,\n",
        "        )\n",
        "\n",
        "        ids = inputs['input_ids']\n",
        "        mask = inputs['attention_mask']\n",
        "\n",
        "        if self.max_len < sum(mask):\n",
        "          i = int(self.max_len / 2)\n",
        "          tmp = copy.copy(ids[:i])\n",
        "          tmp.extend(ids[sum(mask) - i:sum(mask)])\n",
        "          ids = tmp\n",
        "          tmp = copy.copy(mask[:i])\n",
        "          tmp.extend(mask[sum(mask) - i:sum(mask)])\n",
        "          mask = tmp\n",
        "        else:\n",
        "          ids = ids[:self.max_len]\n",
        "          mask = mask[:self.max_len]\n",
        "\n",
        "        y_tensor = torch.Tensor(\n",
        "            [1, 0]) if self.y[index] == 0 else torch.Tensor([0, 1])\n",
        "\n",
        "        return {\n",
        "            'ids': torch.LongTensor(ids),\n",
        "            'mask': torch.LongTensor(mask),\n",
        "            'labels': y_tensor\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vNRZp025gsnp",
        "outputId": "f9f74c9a-8320-4959-f8d0-83a218ffaf1c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ids: tensor([    2,  6918, 28446,  1040, 13162,  1519,    15,    16,  1803, 28188,\n",
            "        28449,     5,    29,  4755,  1845, 28620,    20,    16,  6638,    84,\n",
            "           16, 12097, 17131,     7,    53,    11,  3596,    58,    16, 12844,\n",
            "            7,    58,    16,   106, 13215,     7, 19022,  6172,  4479,  3318,\n",
            "          552,    45,     5,  4545,    12,  4293, 12999,    54,    53,    11,\n",
            "         3596,     7,   139,    18,    54,   171,   695,  2340,  6675, 14209,\n",
            "        28828,  6172,  7105, 11218,  1852,     3,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0])\n",
            "mask: tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
            "labels: tensor([0., 1.])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2307: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        }
      ],
      "source": [
        "# 最大系列長の指定\n",
        "MAX_LEN = 96\n",
        "TEST_MAX_LEN = 128\n",
        "\n",
        "# tokenizerの取得\n",
        "tokenizer = BertJapaneseTokenizer.from_pretrained(model_name)\n",
        "# tokenizer = AlbertTokenizerFast.from_pretrained(model_name)\n",
        "\n",
        "# Datasetの作成\n",
        "dataset_train = CreateDataset(\n",
        "    train_text, y, tokenizer, MAX_LEN)\n",
        "dataset_valid = CreateDataset(\n",
        "    valid_text, valid_y, tokenizer, MAX_LEN)\n",
        "dataset_test = CreateDataset(\n",
        "    test_text, test_y, tokenizer, TEST_MAX_LEN)\n",
        "dataset_compe = CreateDataset(\n",
        "    compe_text, np.zeros(len(compe_text)), tokenizer, TEST_MAX_LEN\n",
        ")\n",
        "\n",
        "for var in dataset_train[3260]:\n",
        "    print(f'{var}: {dataset_train[3260][var]}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8o7bECn_rjfM",
        "outputId": "f3e8ef0c-e962-4bdd-f1ec-557382c15833"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "最大単語数:  150\n"
          ]
        }
      ],
      "source": [
        "# 最大単語数の確認\n",
        "max_len = []\n",
        "# 1文づつ処理\n",
        "for i, sent in enumerate(compe_text):\n",
        "    # Tokenizeで分割\n",
        "    token_words = tokenizer.tokenize(sent)\n",
        "    # 文章数を取得してリストへ格納\n",
        "    max_len.append(len(token_words))\n",
        "    if len(token_words) == 136:\n",
        "      print(i)\n",
        "# 最大の値を確認\n",
        "print('最大単語数: ', max(max_len))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "HK8Oltr1cN6q"
      },
      "outputs": [],
      "source": [
        "def torch_fix_seed(seed=0):\n",
        "    # Python random\n",
        "    random.seed(seed)\n",
        "    # Numpy\n",
        "    np.random.seed(seed)\n",
        "    # Pytorch\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.use_deterministic_algorithms = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "SIY0cZt2Ah8t"
      },
      "outputs": [],
      "source": [
        "class Mixout(InplaceFunction):\n",
        "    @staticmethod\n",
        "    def _make_noise(input):\n",
        "        return input.new().resize_as_(input)\n",
        "\n",
        "    @classmethod\n",
        "    def forward(cls, ctx, input, target=None, p=0.0, training=False, inplace=False):\n",
        "        if p < 0 or p > 1:\n",
        "            raise ValueError(\"A mix probability of mixout has to be between 0 and 1,\" \" but got {}\".format(p))\n",
        "        if target is not None and input.size() != target.size():\n",
        "            raise ValueError(\n",
        "                \"A target tensor size must match with a input tensor size {},\"\n",
        "                \" but got {}\".format(input.size(), target.size())\n",
        "            )\n",
        "        ctx.p = p\n",
        "        ctx.training = training\n",
        "\n",
        "        if ctx.p == 0 or not ctx.training:\n",
        "            return input\n",
        "\n",
        "        if target is None:\n",
        "            target = cls._make_noise(input)\n",
        "            target.fill_(0)\n",
        "        target = target.to(input.device)\n",
        "\n",
        "        if inplace:\n",
        "            ctx.mark_dirty(input)\n",
        "            output = input\n",
        "        else:\n",
        "            output = input.clone()\n",
        "\n",
        "        ctx.noise = cls._make_noise(input)\n",
        "        if len(ctx.noise.size()) == 1:\n",
        "            ctx.noise.bernoulli_(1 - ctx.p)\n",
        "        else:\n",
        "            ctx.noise[0].bernoulli_(1 - ctx.p)\n",
        "            ctx.noise = ctx.noise[0].repeat(input.size()[0], 1)\n",
        "        ctx.noise.expand_as(input)\n",
        "\n",
        "        if ctx.p == 1:\n",
        "            output = target\n",
        "        else:\n",
        "            output = ((1 - ctx.noise) * target + ctx.noise * output - ctx.p * target) / (1 - ctx.p)\n",
        "        return output\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        if ctx.p > 0 and ctx.training:\n",
        "            return grad_output * ctx.noise, None, None, None, None\n",
        "        else:\n",
        "            return grad_output, None, None, None, None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "R244UXceAwuK"
      },
      "outputs": [],
      "source": [
        "def mixout(input, target=None, p=0.0, training=False, inplace=False):\n",
        "    return Mixout.apply(input, target, p, training, inplace)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "ZgzqLWDEKT0-"
      },
      "outputs": [],
      "source": [
        "class MixLinear(torch.nn.Module):\n",
        "    __constants__ = [\"bias\", \"in_features\", \"out_features\"]\n",
        "    def __init__(self, in_features, out_features, bias=True, target=None, p=0.0):\n",
        "        super(MixLinear, self).__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.weight = Parameter(torch.Tensor(out_features, in_features))\n",
        "        if bias:\n",
        "            self.bias = Parameter(torch.Tensor(out_features))\n",
        "        else:\n",
        "            self.register_parameter(\"bias\", None)\n",
        "        self.reset_parameters()\n",
        "        self.target = target\n",
        "        self.p = p\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
        "        if self.bias is not None:\n",
        "            fan_in, _ = init._calculate_fan_in_and_fan_out(self.weight)\n",
        "            bound = 1 / math.sqrt(fan_in)\n",
        "            init.uniform_(self.bias, -bound, bound)\n",
        "\n",
        "    def forward(self, input):\n",
        "        return F.linear(input, mixout(self.weight, self.target, self.p, self.training), self.bias)\n",
        "\n",
        "    def extra_repr(self):\n",
        "        type = \"drop\" if self.target is None else \"mix\"\n",
        "        return \"{}={}, in_features={}, out_features={}, bias={}\".format(\n",
        "            type + \"out\", self.p, self.in_features, self.out_features, self.bias is not None\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "rMerhkSKKUkp"
      },
      "outputs": [],
      "source": [
        "def replace_mixout(model, mixout):\n",
        "    for sup_module in model.modules():\n",
        "        for name, module in sup_module.named_children():\n",
        "            if isinstance(module, torch.nn.Dropout):\n",
        "                module.p = 0.0\n",
        "            if isinstance(module, torch.nn.Linear):\n",
        "                target_state_dict = module.state_dict()\n",
        "                bias = True if module.bias is not None else False\n",
        "                new_module = MixLinear(\n",
        "                    module.in_features, module.out_features, bias, target_state_dict[\"weight\"], mixout\n",
        "                )\n",
        "                new_module.load_state_dict(target_state_dict)\n",
        "                setattr(sup_module, name, new_module)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "pT4lQfIjgusY"
      },
      "outputs": [],
      "source": [
        "# BERT分類モデルの定義\n",
        "class BERTClass(torch.nn.Module):\n",
        "    __constants__ = [\"bias\", \"in_features\", \"out_features\"]\n",
        "    def __init__(self, pretrained, drop_rate, output_size):\n",
        "        super().__init__()\n",
        "        self.output_size = output_size\n",
        "        self.config = BertConfig.from_pretrained(pretrained)\n",
        "        # self.config = BertConfig(classifier_dropout=drop_rate)\n",
        "        self.bert = BertModel.from_pretrained(pretrained)\n",
        "        # self.drop = torch.nn.Dropout(drop_rate)\n",
        "        self.cnn1 = torch.nn.Conv1d(self.config.hidden_size, 256, kernel_size=2)\n",
        "        self.cnn2 = torch.nn.Conv1d(256,  self.output_size , kernel_size=2,  padding=1)\n",
        "        # self.fc = torch.nn.Linear(768, output_size)  # BERTの出力に合わせて768次元を指定\n",
        "\n",
        "    def forward(self, ids, mask, batch_size):\n",
        "        out = self.bert(ids, attention_mask=mask, return_dict=True)\n",
        "        # out = self.fc(self.drop(out))\n",
        "\n",
        "        last_hidden_state = out['last_hidden_state'].permute(0, 2, 1)\n",
        "        # out = out.permute(1, 0).reshape(batch_size, 768, 1)\n",
        "        cnn_embeddings = F.relu(self.cnn1(last_hidden_state))\n",
        "        out = self.cnn2(cnn_embeddings)\n",
        "        out, _ = torch.max(out, 2)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "9ID46VTRgxgo"
      },
      "outputs": [],
      "source": [
        "def calculate_loss_and_accuracy(model, loader, device, batch_size, criterion=None):\n",
        "    \"\"\" 損失・正解率を計算\"\"\"\n",
        "    model.eval()\n",
        "    loss = 0.0\n",
        "    total = 0\n",
        "    correct = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data in loader:\n",
        "            # デバイスの指定\n",
        "            ids = data['ids'].to(device)\n",
        "            mask = data['mask'].to(device)\n",
        "            labels = data['labels'].to(device)\n",
        "\n",
        "            # 順伝播\n",
        "            outputs = model(ids, mask, batch_size)\n",
        "\n",
        "            # 損失計算\n",
        "            if criterion != None:\n",
        "                loss += criterion(outputs, labels).item()\n",
        "\n",
        "            # 正解率計算\n",
        "            # バッチサイズの長さの予測ラベル配列\n",
        "            pred = torch.argmax(outputs, dim=-1).cpu().numpy()\n",
        "            # バッチサイズの長さの正解ラベル配列\n",
        "            labels = torch.argmax(labels, dim=-1).cpu().numpy()\n",
        "            total += len(labels)\n",
        "            correct += (pred == labels).sum().item()\n",
        "\n",
        "    return loss / len(loader), correct / total"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "1k7ULY3qhOtC"
      },
      "outputs": [],
      "source": [
        "def train_model(dataset_train, dataset_valid, batch_size, model, criterion, optimizer, num_epochs, device=None):\n",
        "    \"\"\"モデルの学習を実行し、損失・正解率のログを返す\"\"\"\n",
        "    # デバイスの指定\n",
        "    model.to(device)\n",
        "\n",
        "    # dataloaderの作成\n",
        "    dataloader_train = DataLoader(\n",
        "        dataset_train, batch_size=batch_size, shuffle=True)\n",
        "    dataloader_valid = DataLoader(\n",
        "        dataset_valid, batch_size=len(dataset_valid), shuffle=False)\n",
        "    \n",
        "    scaler = GradScaler()\n",
        "    # ITERS_TO_ACCUMULATE = 2\n",
        "\n",
        "    # 学習\n",
        "    log_train = []\n",
        "    log_valid = []\n",
        "    for epoch in range(num_epochs):\n",
        "        # 開始時刻の記録\n",
        "        s_time = time.time()\n",
        "\n",
        "        # 訓練モードに設定\n",
        "        model.train()\n",
        "        for i, data in enumerate(dataloader_train):\n",
        "            # デバイスの指定\n",
        "            ids = data['ids'].to(device)\n",
        "            mask = data['mask'].to(device)\n",
        "            labels = data['labels'].to(device)\n",
        "\n",
        "            # 勾配をゼロで初期化\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            with autocast():\n",
        "              # 順伝播 + 誤差逆伝播 + 重み更新\n",
        "              outputs = model(ids, mask, batch_size)\n",
        "              loss = criterion(outputs, labels)\n",
        "              # loss = loss / ITERS_TO_ACCUMULATE\n",
        "            \n",
        "            # loss.backward()\n",
        "            scaler.scale(loss).backward() \n",
        "\n",
        "            del loss\n",
        "            \n",
        "            # optimizer.step()\n",
        "            # if (i + 1) % ITERS_TO_ACCUMULATE == 0:\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update() \n",
        "              # optimizer.zero_grad()\n",
        "\n",
        "        # 損失と正解率の算出\n",
        "        loss_train, acc_train = calculate_loss_and_accuracy(\n",
        "            model, dataloader_train, device, batch_size, criterion=criterion)\n",
        "        loss_valid, acc_valid = calculate_loss_and_accuracy(\n",
        "            model, dataloader_valid, device, len(dataset_valid), criterion=criterion)\n",
        "        log_train.append([loss_train, acc_train])\n",
        "        log_valid.append([loss_valid, acc_valid])\n",
        "\n",
        "        # チェックポイントの保存\n",
        "        torch.save({'epoch': epoch, 'model_state_dict': model.state_dict(\n",
        "        ), 'optimizer_state_dict': optimizer.state_dict()}, f'checkpoint{epoch + 1}.pt')\n",
        "\n",
        "        # 終了時刻の記録\n",
        "        e_time = time.time()\n",
        "\n",
        "        # ログを出力\n",
        "        print(f'epoch: {epoch + 1}, loss_train: {loss_train:.4f}, accuracy_train: {acc_train:.4f}, loss_valid: {loss_valid:.4f}, accuracy_valid: {acc_valid:.4f}, {(e_time - s_time):.4f}sec')\n",
        "\n",
        "    return {'train': log_train, 'valid': log_valid}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qwrmnOY3gzyI",
        "outputId": "59048e88-235d-4af9-c9d0-5d272f41efb4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2307: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        }
      ],
      "source": [
        "torch.cuda.empty_cache()\n",
        "torch_fix_seed(SEED)\n",
        "# パラメータの設定\n",
        "DROP_RATE = 0.3\n",
        "OUTPUT_SIZE = 2\n",
        "BATCH_SIZE = 8\n",
        "NUM_EPOCHS = 4\n",
        "LEARNING_RATE = 2e-4\n",
        "MIX_OUT = 0.7\n",
        "\n",
        "# モデルの定義\n",
        "model = BERTClass(model_name, DROP_RATE, OUTPUT_SIZE)\n",
        "model = replace_mixout(model, MIX_OUT)\n",
        "\n",
        "# 損失関数の定義\n",
        "criterion = torch.nn.BCEWithLogitsLoss()\n",
        "\n",
        "# オプティマイザの定義\n",
        "# optimizer = torch.optim.AdamW(params=model.parameters(), lr=LEARNING_RATE)\n",
        "optimizer = torch_optimizer.RAdam(params=model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "# デバイスの指定\n",
        "device = 'cuda' if cuda.is_available() else 'cpu'\n",
        "\n",
        "# モデルの学習\n",
        "log = train_model(dataset_train, dataset_valid, BATCH_SIZE,\n",
        "                  model, criterion, optimizer, NUM_EPOCHS, device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X4U9dX1aefX3"
      },
      "outputs": [],
      "source": [
        "# ログの可視化\n",
        "x_axis = [x for x in range(1, len(log['train']) + 1)]\n",
        "fig, ax = plt.subplots(1, 2, figsize=(15, 5))\n",
        "ax[0].plot(x_axis, np.array(log['train']).T[0], label='train')\n",
        "ax[0].plot(x_axis, np.array(log['valid']).T[0], label='valid')\n",
        "ax[0].set_xlabel('epoch')\n",
        "ax[0].set_ylabel('loss')\n",
        "ax[0].legend()\n",
        "ax[1].plot(x_axis, np.array(log['train']).T[1], label='train')\n",
        "ax[1].plot(x_axis, np.array(log['valid']).T[1], label='valid')\n",
        "ax[1].set_xlabel('epoch')\n",
        "ax[1].set_ylabel('accuracy')\n",
        "ax[1].legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UCoQPF8llOe1"
      },
      "outputs": [],
      "source": [
        "# 正解率の算出\n",
        "dataloader_train = DataLoader(dataset_train, batch_size=1, shuffle=False)\n",
        "dataloader_valid = DataLoader(dataset_valid, batch_size=1, shuffle=False)\n",
        "dataloader_test = DataLoader(dataset_test, batch_size=1, shuffle=False)\n",
        "\n",
        "print(\n",
        "    f'正解率（学習データ）：{calculate_loss_and_accuracy(model, dataloader_train, device, 1)[1]:.3f}')\n",
        "print(\n",
        "    f'正解率（検証データ）：{calculate_loss_and_accuracy(model, dataloader_valid, device, 1)[1]:.3f}')\n",
        "print(\n",
        "    f'正解率（評価データ）：{calculate_loss_and_accuracy(model, dataloader_test, device, 1)[1]:.3f}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataloader_compe = DataLoader(dataset_compe, batch_size=1, shuffle=False)\n",
        "\n",
        "pred = []\n",
        "with torch.no_grad():\n",
        "  for data in dataloader_compe:\n",
        "    # デバイスの指定\n",
        "    ids = data['ids'].to(device)\n",
        "    mask = data['mask'].to(device)\n",
        "\n",
        "    # 順伝播\n",
        "    outputs = model(ids, mask, 1)\n",
        "\n",
        "    # 正解率計算\n",
        "    # バッチサイズの長さの予測ラベル配列\n",
        "    pred_label = torch.argmax(outputs, dim=-1).cpu().numpy()\n",
        "    pred.append(pred_label[0])\n",
        "\n",
        "  csv_data = pd.DataFrame(data=pred, columns=[\"label\"])\n",
        "  csv_data.reset_index(inplace=True)\n",
        "  csv_data = csv_data.rename(columns={'index': 'ID'})\n",
        "  # submission.csvを最大2つまで馬場宛に提出してください．良い方の結果を最終スコアとします．\n",
        "  csv_data.to_csv(submission_path, index=False)"
      ],
      "metadata": {
        "id": "9ELZelkUefyM"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "compe.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}