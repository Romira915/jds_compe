{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Romira915/jds_compe/blob/valid%2Fgpu/compe.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IM5uLIQxeh7q"
      },
      "outputs": [],
      "source": [
        "  !pip install mecab-python3 transformers fugashi ipadic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MlRdjjoxgfeq"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "import MeCab\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import transformers\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch import Tensor, cuda, optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import (AutoModel, AutoTokenizer, BertJapaneseTokenizer,\n",
        "                          BertModel, BartForSequenceClassification)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_q3cDMvEfO1S"
      },
      "outputs": [],
      "source": [
        "train_path = \"train.csv\"\n",
        "test_path = \"test.csv\"\n",
        "compe_path = \"compe.csv\"\n",
        "\n",
        "try:\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')\n",
        "\n",
        "  compe_dir = \"/content/drive/My Drive/Documents/compe/\"\n",
        "\n",
        "  train_path = compe_dir + train_path\n",
        "  test_path = compe_dir + test_path\n",
        "  compe_path = compe_dir + compe_path\n",
        "\n",
        "except ImportError:\n",
        "  pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AG2fQszTgim5"
      },
      "outputs": [],
      "source": [
        "train_df = pd.read_csv(train_path)\n",
        "test_df = pd.read_csv(test_path)\n",
        "compe_df = pd.read_csv(compe_path)\n",
        "\n",
        "train_df, valid_df = train_test_split(\n",
        "    train_df, test_size=0.5, shuffle=True)\n",
        "\n",
        "train_text = train_df[\"text\"].values.astype('U')\n",
        "valid_text = valid_df[\"text\"].values.astype('U')\n",
        "test_text = test_df[\"text\"].values.astype('U')\n",
        "y = train_df[\"label\"].values.astype(\"int8\")\n",
        "valid_y = valid_df[\"label\"].values.astype(\"int8\")\n",
        "test_y = test_df[\"label\"].values.astype(\"int8\")\n",
        "compe_text = compe_df[\"text\"].values.astype('U')\n",
        "\n",
        "# model_name = \"nlptown/bert-base-multilingual-uncased-sentiment\"\n",
        "# model_name = \"daigo/bert-base-japanese-sentiment\"\n",
        "model_name = \"cl-tohoku/bert-base-japanese-whole-word-masking\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ISZnBa7kgnEo"
      },
      "outputs": [],
      "source": [
        "# Datasetの定義\n",
        "class CreateDataset(Dataset):\n",
        "    def __init__(self, X, y, tokenizer, max_len):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):  # len(Dataset)で返す値を指定\n",
        "        return len(self.y)\n",
        "\n",
        "    def __getitem__(self, index):  # Dataset[index]で返す値を指定\n",
        "        text = self.X[index]\n",
        "        inputs = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            pad_to_max_length=True\n",
        "            # padding=True, \n",
        "            # truncation=True,\n",
        "        )\n",
        "        ids = inputs['input_ids']\n",
        "        mask = inputs['attention_mask']\n",
        "\n",
        "        y_tensor = torch.Tensor(\n",
        "            [1, 0]) if self.y[index] == 0 else torch.Tensor([0, 1])\n",
        "\n",
        "        return {\n",
        "            'ids': torch.LongTensor(ids),\n",
        "            'mask': torch.LongTensor(mask),\n",
        "            'labels': y_tensor\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vNRZp025gsnp"
      },
      "outputs": [],
      "source": [
        "# 最大系列長の指定\n",
        "MAX_LEN = 128 +64\n",
        "\n",
        "# tokenizerの取得\n",
        "tokenizer = BertJapaneseTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Datasetの作成\n",
        "dataset_train = CreateDataset(\n",
        "    train_text, y, tokenizer, MAX_LEN)\n",
        "dataset_valid = CreateDataset(\n",
        "    valid_text, valid_y, tokenizer, MAX_LEN)\n",
        "dataset_test = CreateDataset(\n",
        "    test_text, test_y, tokenizer, MAX_LEN)\n",
        "\n",
        "for var in dataset_train[0]:\n",
        "    print(f'{var}: {dataset_train[0][var]}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pT4lQfIjgusY"
      },
      "outputs": [],
      "source": [
        "# BERT分類モデルの定義\n",
        "class BERTClass(torch.nn.Module):\n",
        "    def __init__(self, pretrained, drop_rate, output_size):\n",
        "        super().__init__()\n",
        "        self.bert = BertModel.from_pretrained(pretrained)\n",
        "        self.drop = torch.nn.Dropout(drop_rate)\n",
        "        self.fc = torch.nn.Linear(768, output_size)  # BERTの出力に合わせて768次元を指定\n",
        "\n",
        "    def forward(self, ids, mask):\n",
        "        _, out = self.bert(ids, attention_mask=mask, return_dict=False)\n",
        "        out = self.fc(self.drop(out))\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ID46VTRgxgo"
      },
      "outputs": [],
      "source": [
        "def calculate_loss_and_accuracy(model, loader, device, criterion=None):\n",
        "    \"\"\" 損失・正解率を計算\"\"\"\n",
        "    model.eval()\n",
        "    loss = 0.0\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data in loader:\n",
        "            # デバイスの指定\n",
        "            ids = data['ids'].to(device)\n",
        "            mask = data['mask'].to(device)\n",
        "            labels = data['labels'].to(device)\n",
        "\n",
        "            # 順伝播\n",
        "            outputs = model(ids, mask)\n",
        "\n",
        "            # 損失計算\n",
        "            if criterion != None:\n",
        "                loss += criterion(outputs, labels).item()\n",
        "\n",
        "            # 正解率計算\n",
        "            # バッチサイズの長さの予測ラベル配列\n",
        "            pred = torch.argmax(outputs, dim=-1).cpu().numpy()\n",
        "            # バッチサイズの長さの正解ラベル配列\n",
        "            labels = torch.argmax(labels, dim=-1).cpu().numpy()\n",
        "            total += len(labels)\n",
        "            correct += (pred == labels).sum().item()\n",
        "\n",
        "    return loss / len(loader), correct / total"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(dataset_train, dataset_valid, batch_size, model, criterion, optimizer, num_epochs, device=None):\n",
        "    \"\"\"モデルの学習を実行し、損失・正解率のログを返す\"\"\"\n",
        "    # デバイスの指定\n",
        "    model.to(device)\n",
        "\n",
        "    # dataloaderの作成\n",
        "    dataloader_train = DataLoader(\n",
        "        dataset_train, batch_size=batch_size, shuffle=True)\n",
        "    dataloader_valid = DataLoader(\n",
        "        dataset_valid, batch_size=len(dataset_valid), shuffle=False)\n",
        "\n",
        "    # 学習\n",
        "    log_train = []\n",
        "    log_valid = []\n",
        "    for epoch in range(num_epochs):\n",
        "        # 開始時刻の記録\n",
        "        s_time = time.time()\n",
        "\n",
        "        # 訓練モードに設定\n",
        "        model.train()\n",
        "        for data in dataloader_train:\n",
        "            # デバイスの指定\n",
        "            ids = data['ids'].to(device)\n",
        "            mask = data['mask'].to(device)\n",
        "            labels = data['labels'].to(device)\n",
        "\n",
        "            # 勾配をゼロで初期化\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # 順伝播 + 誤差逆伝播 + 重み更新\n",
        "            outputs = model(ids, mask)\n",
        "            loss = criterion(outputs, labels)\n",
        "            # loss = outputs.loss\n",
        "            loss.backward()\n",
        "\n",
        "            del loss\n",
        "            \n",
        "            optimizer.step()\n",
        "\n",
        "        # 損失と正解率の算出\n",
        "        loss_train, acc_train = calculate_loss_and_accuracy(\n",
        "            model, dataloader_train, device, criterion=criterion)\n",
        "        loss_valid, acc_valid = calculate_loss_and_accuracy(\n",
        "            model, dataloader_valid, device, criterion=criterion)\n",
        "        log_train.append([loss_train, acc_train])\n",
        "        log_valid.append([loss_valid, acc_valid])\n",
        "\n",
        "        # チェックポイントの保存\n",
        "        torch.save({'epoch': epoch, 'model_state_dict': model.state_dict(\n",
        "        ), 'optimizer_state_dict': optimizer.state_dict()}, f'checkpoint{epoch + 1}.pt')\n",
        "\n",
        "        # 終了時刻の記録\n",
        "        e_time = time.time()\n",
        "\n",
        "        # ログを出力\n",
        "        print(f'epoch: {epoch + 1}, loss_train: {loss_train:.4f}, accuracy_train: {acc_train:.4f}, loss_valid: {loss_valid:.4f}, accuracy_valid: {acc_valid:.4f}, {(e_time - s_time):.4f}sec')\n",
        "\n",
        "    return {'train': log_train, 'valid': log_valid}"
      ],
      "metadata": {
        "id": "1k7ULY3qhOtC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qwrmnOY3gzyI"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()\n",
        "# パラメータの設定\n",
        "DROP_RATE = 0.7\n",
        "OUTPUT_SIZE = 2\n",
        "BATCH_SIZE = 8\n",
        "NUM_EPOCHS = 4\n",
        "LEARNING_RATE = 2e-5\n",
        "\n",
        "# モデルの定義\n",
        "model = BERTClass(model_name, DROP_RATE, OUTPUT_SIZE)\n",
        "\n",
        "# 損失関数の定義\n",
        "criterion = torch.nn.BCEWithLogitsLoss()\n",
        "\n",
        "# オプティマイザの定義\n",
        "optimizer = torch.optim.AdamW(params=model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "# デバイスの指定\n",
        "device = 'cuda' if cuda.is_available() else 'cpu'\n",
        "\n",
        "# モデルの学習\n",
        "log = train_model(dataset_train, dataset_valid, BATCH_SIZE,\n",
        "                  model, criterion, optimizer, NUM_EPOCHS, device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X4U9dX1aefX3"
      },
      "outputs": [],
      "source": [
        "# ログの可視化\n",
        "x_axis = [x for x in range(1, len(log['train']) + 1)]\n",
        "fig, ax = plt.subplots(1, 2, figsize=(15, 5))\n",
        "ax[0].plot(x_axis, np.array(log['train']).T[0], label='train')\n",
        "ax[0].plot(x_axis, np.array(log['valid']).T[0], label='valid')\n",
        "ax[0].set_xlabel('epoch')\n",
        "ax[0].set_ylabel('loss')\n",
        "ax[0].legend()\n",
        "ax[1].plot(x_axis, np.array(log['train']).T[1], label='train')\n",
        "ax[1].plot(x_axis, np.array(log['valid']).T[1], label='valid')\n",
        "ax[1].set_xlabel('epoch')\n",
        "ax[1].set_ylabel('accuracy')\n",
        "ax[1].legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UCoQPF8llOe1"
      },
      "outputs": [],
      "source": [
        "# 正解率の算出\n",
        "dataloader_train = DataLoader(dataset_train, batch_size=1, shuffle=False)\n",
        "dataloader_valid = DataLoader(dataset_valid, batch_size=1, shuffle=False)\n",
        "dataloader_test = DataLoader(dataset_test, batch_size=1, shuffle=False)\n",
        "\n",
        "print(\n",
        "    f'正解率（学習データ）：{calculate_loss_and_accuracy(model, dataloader_train, device)[1]:.3f}')\n",
        "print(\n",
        "    f'正解率（検証データ）：{calculate_loss_and_accuracy(model, dataloader_valid, device)[1]:.3f}')\n",
        "print(\n",
        "    f'正解率（評価データ）：{calculate_loss_and_accuracy(model, dataloader_test, device)[1]:.3f}')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "compe.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}